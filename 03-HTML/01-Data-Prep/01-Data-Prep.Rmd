---
title: "Realized Higher Moment Measures"
description: |
  Part 1: Exploratory Data Analysis & Data Preprocessing
author:
  - name: Alexis Solis Cancino
    url: alexis.solisc@gmail.com
    affiliation: ITAM
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 2
    css: "D:/11-Estancia/Finance-Thesis/05-Resources/styles.css"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

# --- set chunk options ---
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  echo = FALSE
)

# --- Data Wrangling ---
library(tidyverse)
library(janitor)
library(arrow)
library(lubridate)
library(hms)
library(readxl)

# --- Modeling Time Series ---
library(timetk)
library(anomalize) # for outlier detection


# --- Paths Management ---
library(here)


# --- Source ggplot2 themes ---
source(here("05-Resources", "ggplot2_themes.R"))
```

# 1. Introduction & Importing Data

We'll work with intraday data for the *S&P/BMV IPC Equity Index*. The data consists of `n = 2,133,890` observations and `k = 23` variables. The time-series is composed of prices and trades per minute, spanning from the beginning of 1996 through the first half of 2018.

```{r, echo = T}
# Read the data
IPC <- arrow::read_parquet(file = here("01-Data", "parquet", "raw_MEXICO_IPC.parquet"))
```

First thing we do is take a look at the columns and data types that we have:

```{r}
IPC %>% glimpse()
```

```{r}
# Create vector with new column names
column_names <- c("ticker","raw_date","raw_time","type",
                  "open","high","low","last","volume",
                  "average_price","vwap","no_trades",
                  "correction_qualifiers","open_bid",
                  "high_bid","low_bid","close_bid",
                  "no_bids","open_ask","high_ask","low_ask",
                  "close_ask","no_ask")

# Rename the columns
IPC <- IPC %>% set_names(column_names)
```

We can count how many `NA`s are present in our data. We do this per column:

```{r}
map_df(IPC, ~sum(is.na(.))) %>% glimpse()
```

We see that there are 10 columns (variables) that have all values as `NA`. We assign these variables to the `columns_to_remove` object and remove them from the data.

```{r}
# Get which columns have all values as NAs.
columns_to_remove <- map_df(IPC, ~ sum(is.na(.))) %>%
  pivot_longer(cols = everything(), 
               names_to = "variable", 
               values_to = "no_missing") %>% 
  filter(no_missing > 0) %>%
  pull(variable)

# Print the selected columns.
columns_to_remove
```

We name the *clean* dataset as `IPC_ip` (*IPC intraday prices*) and again see the column names and each data type.

```{r}
# Get a new dataset without the selected columns
IPC_ip <- IPC %>% select(-all_of(columns_to_remove))

# Remove the older df
rm(IPC)

# Take a look at the data
IPC_ip %>% glimpse()
```

# 2. Feature Engineering & Data Wrangling

We then carry on with the analysis by creating some new variables (a.k.a. *Feature Engineering*) and manipulating the data. The first manipulations we do are the following:

1.  First, a `tidy_date` variable is created, where the date is parsed according to the *ISO 8601* standard that states that dates should be expressed in the `YYYY-MM-DD` format. In consequence, the `raw_date` column is dropped and its replacement will be the newly created `tidy_date` variable.

2.  The `raw_time` column is replaced for the `tidy_time` variable, which parses the time correctly.

3.  The `no_bids`, `no_ask`, `average_price`, `ticker`, `type`, `open`, `high`, and `low` columns are removed because (we think ) they are of no use for the analysis.

4.  We rename the `last` variable as `last_price`.

The modified data is stored into the `IPC_tbl` (*IPC tibble*) object.

```{r}
IPC_tbl <- IPC_ip %>% 
  
  # Create time variables: tidy_date, tidy_time, tidy_dttm
  mutate(tidy_date = lubridate::ymd(raw_date),
         tidy_time = hms::as_hms(raw_time)
         ) %>%
  
  # Remove some columns
  select(-c(raw_date, raw_time, ticker, type, open, high, low, no_bids, no_ask, average_price)) %>% 
  
  # Get newly created variable to the beginning of tibble
  relocate(starts_with("tidy"), .before = 1) %>% 
  
  # Rename "last" variable
  rename(last_price = last)

# Remove the older df
rm(IPC_ip)
```

## 2.1 Narrowing down the time window for prices

First, it's useful to see how many different trading-days we have in our data.

```{r}
IPC_tbl %>% 
  distinct(tidy_date) %>% nrow()
```

So we have `5,613` different trading-days.

It's worth noting that, for the `tidy_time` variable, we have data from `05:32:00` all the way through `20:22:00`.

Let's see the earliest times in our data:

```{r}
IPC_tbl %>% 
  select(tidy_time) %>% 
  unique() %>% 
  arrange(tidy_time)
```

And the latest times in our data:

```{r}
IPC_tbl %>% 
  select(tidy_time) %>% 
  unique() %>% 
  arrange(desc(tidy_time))
```

We can also visualize how many datapoints we have, per hour.

```{r, layout="l-body-outset", fig.width=8, fig.asp=0.618}
IPC_tbl %>% 
  mutate(tidy_hour = as.factor(hour(tidy_time))) %>% 
  count(tidy_hour, sort = T, name = "trades_per_hour") %>% 
  # mutate(tidy_hour = fct_reorder(tidy_hour, .x = trades_per_hour, .desc = T)) %>%
  ggplot(aes(tidy_hour, trades_per_hour)) +
  geom_col(fill = amazing_colors[1]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  labs(
    title = "No. Prices per Hour in the Sample",
    x = "Hour",
    y = NULL
  )
```

From the plot above, we see that most of the prices lie between 8:00AM - 3:00PM. It's interesting that the hours `8` and `15` have fewer datapoints. We can further inspect each one to see the hour-minute components of those trades.

```{r, layout="l-body-outset", fig.width=8, fig.asp=0.618}
IPC_tbl %>% 
  mutate(tidy_hour = as.factor(hour(tidy_time)),
         tidy_minute = minute(tidy_time)) %>%
  filter(tidy_hour == 8) %>% 
  count(tidy_minute, sort = T, name = "trades_per_minute") %>% 
  ggplot(aes(tidy_minute, trades_per_minute)) +
  geom_col(fill = amazing_colors[8]) +
  scale_x_continuous(breaks = seq(0, 60, 5)) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  labs(
    title = "Number of Trades per Minute from 8:00AM-8:59AM",
    x = "Minute",
    y = NULL
  )
```

As expected, the second half-hour is the most active (that is, the period from 8:30AM - 8:59AM).

Now, let's do the same for the `15` hour-mark:

```{r, layout="l-body-outset", fig.width=8, fig.asp=0.618}
IPC_tbl %>% 
  mutate(tidy_hour = as.factor(hour(tidy_time)),
         tidy_minute = minute(tidy_time)) %>%
  filter(tidy_hour == 15) %>% 
  count(tidy_minute, sort = T, name = "trades_per_minute") %>% 
  ggplot(aes(tidy_minute, trades_per_minute)) +
  geom_col(fill = amazing_colors[8]) +
  scale_x_continuous(breaks = seq(0, 60, 5)) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
    labs(
    title = "Number of Trades per Minute from 3:00PM-3:59PM",
    x = "Minute",
    y = NULL
  )
```

Here, the time with most trades is 3:00PM exactly. That's what we need because we'll ignore the rest of the trades. Following *Liu, Patton, Sheppard (2013)*, we'll discard any prices that were quoted outside normal business hours. Hence, we want prices that lie between `08:30` and `15:00`.

To achieve this, we'll introduce the concept of a *time series index*.

### Time Series Index

Before we continue, we must create the so called **time series index**. To achieve this, a `dttm` (*datetime*) variable is created, which is the result of combining the `tidy_date` variable and the `tidy_time` variable. This combination gets the name of `tidy_dttm`. In this case, this is the variable by which the time series will be indexed. Below we print a small sample (10 rows) of this variable index:

```{r}
IPC_tbl <- IPC_tbl %>% 
  
  # Add time series index: tidy_dttm
  mutate(tidy_dttm = ymd_hms(str_c(tidy_date, tidy_time))) %>% 
  
  relocate(tidy_dttm, .before = 1)

IPC_tbl %>% select(tidy_dttm)
```

Next, in order to know the *edges*, so to speak, we can get the first and last values in the time series. We name these values `left_edge` (first value) and `right_edge` (last value), respectively.

```{r}
left_edge <- IPC_tbl %>% 
  
  # Arrange in descending tidy_dttm order
  arrange(tidy_dttm) %>% 
  
  # Get the vector
  pull(tidy_dttm) %>% 
  
  # Get the first element of the vector
  first()

right_edge <- IPC_tbl %>% 
  
  # Arrange in descending tidy_dttm order
  arrange(tidy_dttm) %>% 
  
  # Get the vector
  pull(tidy_dttm) %>% 
  
  # Get the last element of the vector
  last()

paste("The first date-time in our data is:", left_edge) 
paste("The last date-time in our data is:", right_edge) 
```

These values are incorrect; the first value in our time-series should be `1996-01-02 08:30:00` and the last one should be: `2018-06-05 15:00:00`. The Time Series Index will be defined based on the *correct* values. These will be created as the `first_dttm` and `last_dttm` variables.

```{r}
first_dttm <- left_edge %>% 
  as_date() %>% 
  paste("08:30:00") %>% 
  as_datetime()

last_dttm <- right_edge %>% 
  as_date() %>% 
  paste("15:00:00") %>% 
  as_datetime()
```

### Adjustments to the Time Series Index

Below, we print the first 10 values of the `ts_index` object (the *time series index*). We also print the last 10 values.

```{r}
# Make index by using vectorized function
ts_index <- tibble(
  
  index = timetk::tk_make_timeseries(start_date = first_dttm,
                                     end_date = last_dttm,
                                     by = "60 sec")
) %>% 
  
  # Create date and time columns
  mutate(index_date = as_date(index),
         
         index_time = hms(seconds = second(index),
                          minutes = minute(index),
                          hours = hour(index)
                          )
         ) %>% 
  
  # Filter for times between 08:30 and 15:00
  filter(index_time >= as_hms("08:30:00"),
         index_time <= as_hms("15:00:00")) %>% 
  
  # Get rid of absent dates  
  filter(index_date %in% IPC_tbl$tidy_date)

# Print first 10 values
ts_index

# Print last 10 values
ts_index %>% 
  arrange(desc(index))
```

Note that we've added two columns to our time series index: `index_date`, and `index_time`. We also removed the dates that don't match those from the original data. This gets rid of weekends, too.

### Joining the Data

We will make some checks further down the line, but for now, the data is ready to be **joined** into a new object, so that we have **no gaps** in between the 1-minute prices.

```{r}
joined_tbl <- ts_index %>% 
  left_join(
    IPC_tbl,
    by = c("index" = "tidy_dttm")) %>% 
  
  # Remove useless columns
  select(-c(tidy_date, tidy_time))
```

Below, we print the first 10 rows of this joined data. It's evident that many prices have an `NA` value, because they're not available. We'll *impute* these next.

```{r}
joined_tbl
```

## 2.2 Price Imputation

Except for the first 6 datapoints, we will impute the prices by looking at the last available price. But first, we do impute those 6 datapoints:

```{r}
joined_tbl$last_price[1:6] <- joined_tbl$last_price[7]

joined_tbl
```

Now the `last_price` column has values for the 6 first minutes. Next, we impute the rest of the prices. We check that the `last_price` column has no missing values next.

```{r}
full_IPC <- joined_tbl %>% 
  fill(last_price)
  
full_IPC %>% map_df(.f = ~ sum(is.na(.)))

full_IPC
```

As we can see, only the `volume` and `no_trades` (number of trades) have missing values now.

Another check we can do is to count the number of prices per unique date. We can do this by just counting the frequency of each date. It must have a frequency of 391, since we have:

\\begin{equation} \text{prices/day} = \text{hours} \times \\text{minutes} + 1 = (6.5) \times 60 + 1 = 391 \\end{equation}

```{r}
full_IPC %>% 
  count(index_date, name = "date_count")
```

Printing the first 10 counts for each date... this looks correct. But now, let's get the unique values from that `date_count` column just to be sure:

```{r}
full_IPC %>% 
  count(index_date, name = "date_count") %>% 
  distinct(date_count)
```

The time series has been successfully padded.

## 2.3 Time Series Signature

We can then extract several time-related variables from the time series index. In this case, we'll get *20 different time-related variables*. We'll call this **The Time Series Signature**, which is a collection of time-related variables that will help for feature engineering and modeling. The most important ones are:

-   `index`: The time-index variable that is being decomposed.
-   `index_date`: The date component of the `index` variable.
-   `index_time`: The time component of the `index` variable.
-   `index_num`: The numeric value of the time series index (in seconds). The base is `1970-01-01 00:00:00` which has the value of 0.
-   `diff`: The difference (in seconds) from the previous numeric `index` value.
-   `year`: The year of the *time series* `index`.
-   `half`: The *half component* of the index (i.e. to which semester does the date belong to).
-   `quarter`: The *quarter component* of the index (i.e. to which quarter does the date belong to).
-   `month`: The *month component* of the index (with base 1 - that is, January = 1 and so on).
-   `month_label`: The three-letter month label as an ordered categorical variable. It begins with *Jan* and ends with *Dec*.
-   `day`: The *day* component of the `index`.
-   `hour`: The *hour* component of the `index` (24-hour scale).
-   `minute`: The *minute* component of the `index` (from 0 - 59).
-   `wday`: The day of the week with base 1. Monday = 1 and Sunday = 7.
-   `wday_label`: The three-letter label for day of the week as an ordered categorical variable. It begins with `Mon` and ends with `Sun`.
-   `qday`: The day of the quarter.
-   `yday`: The day of the year.
-   `mweek`: The week of the month.
-   `week`: The week number of the year.
-   `mday7`: The integer division of the day of the month by seven, which returns the *nth* instance the day has appeared in that month.
-   `date_change`: A *boolean* variable. `TRUE` indicates that the date has changed against the previous index value. `FALSE` indicates no change.

```{r}
IPC_signature <- full_IPC %>% 
  
  # Create time series signature
  mutate(
    index_num   = as.numeric(index),
    diff        = index_num - lag(index_num),
    year        = year(index),
    half        = semester(index, with_year = FALSE),
    quarter     = quarter(index),
    month       = month(index),
    month_label = month(index, label = TRUE),
    day         = day(index),
    hour        = hour(index),
    minute      = minute(index),
    wday        = wday(index, week_start = 1),
    wday_label  = wday(index, label = TRUE),
    qday        = qday(index),
    yday        = yday(index),
    # mweek       =
    # week        = lubridate::week(),
    # mday7       = day %% 7,
    date_change = if_else(index_date == lag(index_date), FALSE, TRUE)
  )
```

Lastly, we do a quick check for:

1.  The time series signature doesn't contain weekend days. We do this by printing the unique values for the weekday label stored in the `wday_label` variable.

```{r}
IPC_signature %>% 
  distinct(wday_label)
```

2.  The time signatures only contains hours from `08:00` through `15:00`. We print the first 10 unique values of the `index_time` variable. We also print the last 10 unique values.

```{r}
# First 10 unique values
IPC_signature %>% 
  distinct(index_time)

# Last 10 unique values
IPC_signature %>% 
  arrange(-index_time) %>% 
  distinct(index_time)
```

## 2.4 Computing Intraday Log-Returns

Next, we compute the intraday returns and assign them to the `log_ret` variable. We also convert our data into a time-friendly type of object called `tibble time` (we do this via the `as_tbl_time()` function).

```{r}
returns_tbl <- IPC_signature %>%

    mutate(log_ret = log(last_price) - lag(log(last_price))) %>% 
  
  relocate(log_ret, .after = 4)
```

```{r}
# Remove the older df
rm(full_IPC)
rm(ts_index)
rm(joined_tbl)
rm(IPC_tbl)
```

## 2.5 Setting Overnight Returns to Zero

Another aspect that we want to take into account is: the fact that the overnight return should not be taken into account. Therefore, we set the first log-return of each day to zero. Once again, we store these values in a new object called `nor_tbl` (which stands for *no overnight returns tibble*).

```{r}
nor_tbl <- returns_tbl %>% 
   
  mutate(log_ret = ifelse(test = date_change == FALSE,
                          yes = log_ret, 
                          no = 0))
# Remove old df
# rm(IPC_tfr_tbl)
```

Our first log-return should be zero, but we have `NA`. Let's check that this is the only `NA`:

```{r}
nor_tbl %>% 
  filter(is.na(log_ret)) %>% 
  select(index_date, log_ret)
```

It is indeed our only `NA`. We can set it to zero.

```{r}
nor_tbl$log_ret[1] <- 0

nor_tbl
```

We can check that all overnight returns are zero. The variable `date_change` from the *time series index* is useful here. We extract the distinct values of the `log_ret` variable along with the `date_change` variable. We should get that all values for `date_change` are `TRUE`, and all values for `log_ret` are `0`.

```{r}
nor_tbl %>% 
  filter(date_change == 1) %>% 
  distinct(log_ret, date_change) 
```

Moreover, the number of overnight returns that have been set to zero should be one less than the number of different dates we have for the `tidy_date` variable. Let's check if this holds true:

```{r}
nor_tbl %>% 
  filter(date_change == TRUE) %>% 
  select(index_date, log_ret, date_change)
```

We have `5,612` datapoints where `date_change = TRUE`. This should be one less than the amount of different dates (one less because the first date doesn't have `date_change` set to one).

```{r}
nor_tbl %>% distinct(index_date) %>% nrow()
```

Thus, we see that this holds true. We have correctly set the overnight returns to zero.

```{r plotting-returns, cache = T}
returns_plot <- nor_tbl %>% 
  ggplot(aes(index, log_ret)) +
  geom_point(aes(alpha = .005,
                 colour = log_ret >= 0), 
             show.legend = FALSE) +
  scale_color_manual(values = amazing_colors[c(2,8)]) +
  labs(
    title = "Visualizing 1-minute log-returns for the S&P/BMV IPC",
    x = NULL,
    y = NULL
  ) +
  scale_x_datetime(date_breaks = "2 years", 
                   date_labels = "%Y") +
  scale_y_continuous(breaks = seq(-0.05, 0.05, by = 0.01),
                     labels = scales::percent_format(accuracy = 0.1)) +
  theme(
    plot.background = element_rect(fill = "#F4F3EE")
  )

returns_plot
```

```{r}
# ggsave(
#   filename = "IPC_returns.png",
#   units = "in",
#   width = 8 * 1.61803398875,
#   height = 8,
#   path = here("02-Plots"),
#   dpi = 500,
#   device = "png"
# )
```

## 2.6 Removing Outliers

We want to filter out log-returns whose absolute value is greater than some threshold. We'll do this again later for 5-minute returns and 30-minute returns. We define the threshold such that we are filtering keeping \~ 99% of the original data. Log-returns that are outside of the defined threshold are immediately set to zero.

```{r}
threshold <- 0.02

IPC_fr <- nor_tbl %>% 
  
  # we keep values below the threshold
  mutate(log_ret = ifelse(test = abs(log_ret) < threshold, 
                          yes = log_ret, 
                          no = 0)
         )

# Remove old df
# rm(IPC_oar_tbl)
```

In this case we've set the variable `threshold` to the value (`0.0012`) such that we're filtering out \~ 1% of the data (25,326 values). We store the filtered data in the `IPC_fr` tibble (which stands for IPC filtered returns).

```{r}
returns_plot2 <- IPC_fr %>% 
  ggplot(aes(index, log_ret)) +
  geom_point(aes(alpha = .005,
                 colour = log_ret >= 0), 
             show.legend = FALSE) +
  scale_color_manual(values = amazing_colors[c(2,8)]) +
  labs(
    title = "Visualizing 1-minute log-returns for the S&P/BMV IPC",
    x = NULL,
    y = NULL
  ) +
  scale_x_datetime(date_breaks = "2 years", 
                   date_labels = "%Y") +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +
  theme(
    plot.background = element_rect(fill = "#F4F3EE")
  )

returns_plot2
```

```{r}
# Write data to parquet
IPC_fr %>% 
  write_parquet(sink = here("01-Data", "parquet", "IPC_fr2.parquet"))
```

# IPC and Volatility Indices -- Daily Data

The data was taken from a Bloomberg terminal and from [S&P Dow Jones Indices](https://spindices.com/indices/strategy/sp-bmv-ipc-vix).

```{r}
indices_data <- 
  
  # Read Excel file
  read_excel(path = here("01-Data", "mexbol-vimex.xlsx"),
             
             # Specify values for NA
             na = c("#NA", "#N/A N/A"),
             
             # Specify column type
             col_types = c("date", rep("numeric", 3))
             ) %>%
  
  # Set names for columns
  set_names(c("date", "mexbol_ipc", "vimex", "ipc_vix")) %>% 
  
  # Parse date column
  mutate(date = ymd(date))

indices_data
```

```{r}
# To-Do:

# 1. Transform data to long format

# 2. Re-do plots with facet_wrap
```

```{r}
indices_data %>% 
  pivot_longer(cols = contains("x"), )
```

Let's visualize the daily data for the IPC and the VIMEX:

```{r, layout="l-body-outset"}
# Build IPC plot
plot_ipc <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & VIMEX Volatility Index",
    subtitle = "(2005 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )


# Build VIMEX plot
plot_vimex <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, vimex)) +
  geom_line(colour = amazing_colors[2]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
    labs(
    x = NULL,
    y = "VIMEX"
  )

# Print plot
cowplot::plot_grid(plot_ipc, 
                   plot_vimex, 
                   ncol = 1,
                   rel_heights = c(1, 1))
```

We can do the same for the IPC and the IPC VIX:

```{r, layout="l-body-outset"}
# Build IPC plot with new title
plot_ipc2 <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & IPC VIX Volatility Index",
    subtitle = "(2015 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )
  

# Build IPC VIX plot
plot_ipcvix <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, ipc_vix)) +
  geom_line(colour = amazing_colors[2]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    x = NULL,
    y = "IPC VIX"
  )

# Print plot
cowplot::plot_grid(plot_ipc2, 
                   plot_ipcvix, 
                   ncol = 1,
                   rel_heights = c(1, 1))
```

We can repeat the plots but with a smooth average on top, so we see the trend along time:

```{r, layout="l-body-outset"}
# Build IPC plot
plot_ipc <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  geom_smooth(colour = amazing_colors[3]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & VIMEX Volatility Index",
    subtitle = "(2005 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )


# Build VIMEX plot
plot_vimex <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, vimex)) +
  geom_line(colour = amazing_colors[2]) +
  geom_smooth(colour = amazing_colors[4]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
    labs(
    x = NULL,
    y = "VIMEX"
  )

# Print plot
cowplot::plot_grid(plot_ipc, 
                   plot_vimex, 
                   ncol = 1,
                   rel_heights = c(1, 1))
```

```{r, layout="l-body-outset"}
# Build IPC plot with new title
plot_ipc2 <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  geom_smooth(colour = amazing_colors[3]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & IPC VIX Volatility Index",
    subtitle = "(2015 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )
  

# Build IPC VIX plot
plot_ipcvix <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, ipc_vix)) +
  geom_line(colour = amazing_colors[2]) +
  geom_smooth(colour = amazing_colors[4]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    x = NULL,
    y = "IPC VIX"
  )

# Print plot
cowplot::plot_grid(plot_ipc2, 
                          plot_ipcvix, 
                          ncol = 1,
                          rel_heights = c(1, 1))
```
