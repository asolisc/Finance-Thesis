---
title: "Realized Higher Moment Measures"
description: |
  Part 1: Exploratory Data Analysis & Data Preprocessing
author:
  - name: Alexis Solis Cancino
    url: alexis.solisc@gmail.com
    affiliation: ITAM
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 2
    css: "D:/11-Estancia/Finance-Thesis/05-Resources/styles.css"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

# --- set chunk options ---
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  echo = FALSE
)

# --- Data Wrangling ---
library(tidyverse)
library(janitor)
library(arrow)
library(lubridate)
library(hms)
library(readxl)

# --- Modeling Time Series ---
library(timetk)
library(anomalize) # for outlier detection


# --- Paths Management ---
library(here)


# --- Source ggplot2 themes ---
source(here("05-Resources", "ggplot2_themes.R"))
```

# 1. Introduction & Importing Data

We'll work with intraday data for the *S&P/BMV IPC Equity Index*. The data consists of `n = 2,133,890` observations and `k = 23` variables. The time-series is composed of prices and trades per minute, spanning from the beginning of 1996 through the first half of 2018.

```{r, echo = T}
# Read the data
IPC <- arrow::read_parquet(file = here("01-Data", "parquet", "raw_MEXICO_IPC.parquet"))
```

First thing we do is take a look at the columns and data types that we have:

```{r}
IPC %>% glimpse()
```

```{r}
# Create vector with new column names
column_names <- c("ticker","raw_date","raw_time","type",
                  "open","high","low","last","volume",
                  "average_price","vwap","no_trades",
                  "correction_qualifiers","open_bid",
                  "high_bid","low_bid","close_bid",
                  "no_bids","open_ask","high_ask","low_ask",
                  "close_ask","no_ask")

# Rename the columns
IPC <- IPC %>% set_names(column_names)
```

We can count how many `NA`s are present in our data. We do this per column:

```{r}
map_df(IPC, ~sum(is.na(.))) %>% glimpse()
```

We see that there are 10 columns (variables) that have all values as `NA`. We assign these variables to the `columns_to_remove` object and remove them from the data.

```{r}
# Get which columns have all values as NAs.
columns_to_remove <- map_df(IPC, ~ sum(is.na(.))) %>%
  pivot_longer(cols = everything(), 
               names_to = "variable", 
               values_to = "no_missing") %>% 
  filter(no_missing > 0) %>%
  pull(variable)

# Print the selected columns.
columns_to_remove
```

We name the *clean* dataset as `IPC_ip` (*IPC intraday prices*) and again see the column names and each data type.

```{r}
# Get a new dataset without the selected columns
IPC_ip <- IPC %>% select(-all_of(columns_to_remove))

# Remove the older df
rm(IPC)

# Take a look at the data
IPC_ip %>% glimpse()
```

# 2. Feature Engineering & Data Wrangling

We then carry on with the analysis by creating some new variables (a.k.a. *Feature Engineering*) and manipulating the data. The first manipulations we do are the following:

1.  First, a `tidy_date` variable is created, where the date is parsed according to the *ISO 8601* standard that states that dates should be expressed in the `YYYY-MM-DD` format. In consequence, the `raw_date` column is dropped and its replacement will be the newly created `tidy_date` variable.

2.  The `raw_time` column is replaced for the `tidy_time` variable, which parses the time correctly.

3.  The `no_bids`, `no_ask`, `average_price`, `ticker`, `type`, `open`, `high`, and `low` columns are removed because (we think ) they are of no use for the analysis.

4.  We rename the `last` variable as `last_price`.

The modified data is stored into the `IPC_tbl` (*IPC tibble*) object.

```{r}
IPC_tbl <- IPC_ip %>% 
  
  # Convert to tibble
  as_tibble() %>%
  
  # Create time variables: tidy_date, tidy_time, tidy_dttm
  mutate(tidy_date = lubridate::ymd(raw_date),
         tidy_time = hms::as_hms(raw_time)
         ) %>%
  
  # Remove some columns
  select(-c(raw_date, raw_time, ticker, type, open, high, low, no_bids, no_ask, average_price)) %>% 
  
  # Get newly created variable to the beginning of tibble
  relocate(starts_with("tidy"), .before = 1) %>% 
  
  # Rename "last" variable
  rename(last_price = last)

# Remove the older df
rm(IPC_ip)
```

### Time Series Index

Before we continue, we must create the so called **time series index**. To achieve this, a `dttm` (*datetime*) variable is created, which is the result of combining the `tidy_date` variable and the `tidy_time` variable. This combination gets the name of `tidy_dttm`. In this case, this is the variable by which the time series will be indexed. Below we print a small sample (10 rows) of this variable index:

```{r}
IPC_tbl <- IPC_tbl %>% 
  
  # Add time series index: tidy_dttm
  mutate(tidy_dttm = ymd_hms(str_c(tidy_date, tidy_time))) %>% 
  
  relocate(tidy_dttm, .before = 1)

IPC_tbl %>% select(tidy_dttm)
```

Next, in order to know the *edges*, so to speak, we can get the first and last values in the time series. We name these values `left_edge` (first value) and `right_edge` (last value), respectively.

```{r}
left_edge <- IPC_tbl %>% 
  
  # Arrange in descending tidy_dttm order
  arrange(tidy_dttm) %>% 
  
  # Get the vector
  pull(tidy_dttm) %>% 
  
  # Get the first element of the vector
  first()

right_edge <- IPC_tbl %>% 
  
  # Arrange in descending tidy_dttm order
  arrange(tidy_dttm) %>% 
  
  # Get the vector
  pull(tidy_dttm) %>% 
  
  # Get the last element of the vector
  last()

paste("The first date-time in our data is:", left_edge) 
paste("The last date-time in our data is:", right_edge) 
```

These values are incorrect; the first value in our time-series should be `1996-01-02 08:30:00` and the last one should be: `2018-06-05 15:00:00`. The Time Series Index will be defined based on the *correct* values. These will be created as the `first_dttm` and `last_dttm` variables.

```{r}
first_dttm <- left_edge %>% 
  str_sub(1, 10) %>% 
  paste("08:30:00") %>% 
  as_datetime()

last_dttm <- right_edge %>% 
  str_sub(1, 10) %>% 
  paste("15:00:00") %>% 
  as_datetime()
```

### Adjustments to the Time Series Index

Below, we print the first 10 values of the `ts_index` object (the *time series index*). We also print the last 10 values.

```{r}
# Make index by using vectorized function
ts_index <- tibble(
  
  index = timetk::tk_make_timeseries(start_date = first_dttm,
                                     end_date = last_dttm,
                                     by = "60 sec")
  ) %>% 
  
  # Create date and time columns
  mutate(index_date = as_date(index),
         index_time = str_sub(index, -8, -1) %>% as_hms()) %>% 
    
    # Filter for times between 08:30 and 15:00
    filter(index_time >= as_hms("08:30:00"),
           index_time <= as_hms("15:00:00")) %>% 
  
  # Get rid of absent dates  
  filter(index_date %in% IPC_tbl$tidy_date)

# Print first 10 values
ts_index

# Print last 10 values
ts_index %>% 
  arrange(desc(index))
```

Note that we've added two columns to our time series index: `index_date`, and `index_time`. We also removed the dates that don't match those from the original data. This gets rid of weekends, too.

### Joining the Data

We will make some checks further down the line, but for now, the data is ready to be **joined** into a new object, so that we have **no gaps** in between the 1-minute prices.

```{r}
joined_tbl <- ts_index %>% 
  left_join(
    IPC_tbl,
    by = c("index" = "tidy_dttm")) %>% 
  
  # Rename the index column
  rename("tidy_dttm" = "index") %>% 
  
  # Remove useless columns
  select(-c(tidy_date, tidy_time))
```

Below, we print the first 10 rows of this joined data. It's evident that many prices have an `NA` value, because they're not available. We'll *impute* these next.

```{r}
joined_tbl
```

### Price Imputation

### Time Series Signature

We can then extract several time-related variables from this time series index. In this case, we'll get *20 different time-related variables*. We'll call this **The Time Series Signature**, which is a collection of time-related variables that will help for feature engineering and modeling. The most important ones are:

-   `index`: The time-index variable that is being decomposed.
-   `index_date`: The date component of the `index` variable.
-   `index_time`: The time component of the `index` variable.
-   `index_num`: The numeric value of the time series index (in seconds). The base is `1970-01-01 00:00:00` which has the value of 0.
-   `diff`: The difference (in seconds) from the previous numeric `index` value.
-   `year`: The year of the *time series* `index`.
-   `half`: The *half component* of the index (i.e. to which semester does the date belong to).
-   `quarter`: The *quarter component* of the index (i.e. to which quarter does the date belong to).
-   `month`: The *month component* of the index (with base 1 - that is, January = 1 and so on).
-   `month_label`: The three-letter month label as an ordered categorical variable. It begins with *Jan* and ends with *Dec*.
-   `day`: The *day* component of the `index`.
-   `hour`: The *hour* component of the `index` (24-hour scale).
-   `minute`: The *minute* component of the `index` (from 0 - 59).
-   `wday`: The day of the week with base 1. Monday = 1 and Sunday = 7.
-   `wday_label`: The three-letter label for day of the week as an ordered categorical variable. It begins with `Mon` and ends with `Sun`.
-   `qday`: The day of the quarter.
-   `yday`: The day of the year.
-   `mweek`: The week of the month.
-   `week`: The week number of the year.
-   `mday7`: The integer division of the day of the month by seven, which returns the *nth* instance the day has appeared in that month.
-   `date_change`: A *boolean* variable. `TRUE` indicates that the date has changed against the previous index value. `FALSE` indicates no change.

```{r}
ts_signature <- ts_index %>% 
  
  # Create time series signature
  mutate(
    index_date  = as_date(index),
    index_time  = str_sub(index, -8, -1) %>% as_hms(),
    # index_num   = as.numeric(index),
    diff        = as.numeric(index) - lag(as.numeric(index)),
    year        = year(index),
    # half        = semester(index, with_year = FALSE),
    # quarter     = quarter(index),
    month       = month(index),
    # month_label = month(index, label = TRUE),
    day         = day(index),
    hour        = hour(index),
    minute      = minute(index),
    # wday        = wday(index, week_start = 1),
    wday_label  = wday(index, label = TRUE),
    # qday        = qday(index),
    # yday        = yday(index),
    # mweek       =
    # week        = lubridate::week(),
    # mday7       = day %% 7,
    date_change = if_else(index_date == lag(index_date), FALSE, TRUE)
  )
```

```{r}
# Write csv file for Time Series Signature:
ts_signature2 %>% write_csv(here("01-Data", "ts_signature.csv"))

# Write csv file for IPC:
IPC_tbl %>% write_csv(here("01-Data", "IPC_csv.csv"))
```

Lastly, we do a quick check for:

1.  The time series signature doesn't contain weekend days. We do this by printing the unique values for the weekday label stored in the `wday_label` variable.

```{r}
ts_signature2 %>% 
  distinct(wday_label)
```

2.  The time signatures only contains hours from `08:00` through `15:00`. We print the first 10 unique values of the `index_time` variable. We also print the last 10 unique values.

```{r}
# First 10 unique values
ts_signature2 %>% 
  distinct(index_time)

# Last 10 unique values
ts_signature2 %>% 
  arrange(-index_time) %>% 
  distinct(index_time)
```

3.  The number of distinct dates in the original dataset is equal to the number of different dates in the time series signature.

```{r}
rows_signature <- ts_signature2 %>% 
  distinct(index_date) %>% 
  nrow()

rows_prices <- IPC_tbl %>% 
  distinct(tidy_date) %>% 
  nrow()

paste('The difference in unique dates from the datasets is:', 
      abs(rows_signature - rows_prices))
```

### Augmenting the Time Series

Next, we add the time series signature to our original data.

Note: this task had to be done in a distributed computing environment with help of the `sparklyr` library, owing to the sheer amount of RAM memory the `join` operations needed.

```{r}
source(here("spark_code.R"))
```

```{r}
directory_path <- here("01-Data", "joined.csv")

# Get each file's full path
joined_IPC <- dir(directory_path, full.names = T) %>%
  map_df(read_csv) %>% 
  
  # Turn into tibble
  as_tibble() %>%
  
  # Re-parse index_date correctly
  mutate(index_date = as_date(index)) %>% 
  
  # Remove duplicate dttm column
  select(-c(tidy_dttm)) %>% 
  
  relocate(last_price, .after = 1)

```

```{r}

# Inspect why table has lots of problems
joined_IPC %>% filter(is.na(index))
```

```{r}
IPC_tbl <- 
  
  IPC_tbl %>%
  
  # Create the time-related variables
  mutate(
    tidy_year   = lubridate::year(tidy_date),
    tidy_month  = lubridate::month(tidy_date),
    tidy_mday   = lubridate::mday(tidy_date),
    tidy_wday   = lubridate::wday(tidy_date, label = T, locale = "English"),
    tidy_hour   = lubridate::hour(tidy_time),
    tidy_minute = lubridate::minute(tidy_time),
    
    # Variables to keep track of time jumps
    date_num    = as.numeric(tidy_dttm),
    diff        = date_num - lag(date_num)
  ) %>%
  
  # Create the trade_id variable
  mutate(trade_id = seq(1:nrow(IPC))) %>%
  
  # Rearrange the order of the columns:
  select(trade_id, tidy_dttm, diff, tidy_date, tidy_time, dplyr::everything())


# Finally take a thorough look at the data:
# IPC_tbl %>% skimr::skim()
```

## Padding the Time Series

Next, we need to pad the time series to fill in the gaps. We will use the **nearest past price** to fill the data for a 1-minute grid. The only exception are the first six prices (which are missing), where we will use the **nearest future price**.

## 2.1 Computing Intraday Log-Returns

Next, we compute the intraday returns and assign them to the `log_ret` variable. We also convert our data into a time-friendly type of object called `tibble time` (we do this via the `as_tbl_time()` function).

```{r}
(IPC_ret_tbl <-
  IPC_tbl %>%
  dplyr::mutate(log_ret = log(last_price) - lag(log(last_price))) %>%
   
  # The following lines filter out the returns with NA:
  # dplyr::filter(!is.na(log_ret)) %>%
  # na.omit() %>%
  
  # The following line filters out the rows with zero returns:
  # dplyr::filter(log_ret != 0) %>%
  
  # Rearrange the order of the columns:
  dplyr::select(trade_id, tidy_date, log_ret, dplyr::everything()) %>%
  
  # Convert to tibbletime
  tibbletime::as_tbl_time(index = tidy_date))

# Remove the older df
# rm(IPC_tbl)
```

## 2.2 Narrowing down the time window for trades

First, it's useful to see how many different trading-days we have in our data.

```{r}
IPC_ret_tbl %>% 
  distinct(tidy_date) %>% nrow()
```

So we have `5,613` different trading- days.

It's worth noting that, for the `tidy_time` variable, we have data from `05:32:00` all the way through `20:22:00`.

Let's see the earliest times in our data:

```{r}
IPC_ret_tbl %>% select(tidy_time) %>% unique() %>% arrange(tidy_time) %>% head()
```

And the latest times in our data:

```{r}
IPC_ret_tbl %>% select(tidy_time) %>% unique() %>% arrange(tidy_time) %>% tail()
```

We can also visualize how many datapoints we have, per hour.

```{r, layout="l-body-outset", fig.width=8, fig.asp=0.618}
IPC_ret_tbl %>% mutate(tidy_hour = as.factor(tidy_hour)) %>% 
  count(tidy_hour, sort = T, name = "trades_per_hour") %>% 
  # mutate(tidy_hour = fct_reorder(tidy_hour, .x = trades_per_hour, .desc = T)) %>%
  ggplot(aes(tidy_hour, trades_per_hour)) +
  geom_col(fill = amazing_colors[1]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  labs(
    title = "No. Prices per Hour in the Sample",
    x = "Hour",
    y = NULL
  )
```

From the plot above, we see that most of the prices lie between 8:00AM - 3:00PM. It's interesting that the hours `8` and `15` have fewer datapoints. We can further inspect each one to see the hour-minute components of those trades.

```{r, layout="l-body-outset", fig.width=8, fig.asp=0.618}
IPC_ret_tbl %>% 
  filter(tidy_hour == 8) %>% 
  count(tidy_minute, sort = T, name = "trades_per_minute") %>% 
  ggplot(aes(tidy_minute, trades_per_minute)) +
  geom_col(fill = amazing_colors[2]) +
  scale_x_continuous(breaks = seq(0, 60, 5)) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  labs(
    title = "Number of Trades per Minute from 8:00AM-8:59AM",
    x = "Minute",
    y = NULL
  )
```

As expected, the second half-hour is the most active (that is, the period from 8:30AM - 8:59AM).

Now, let's do the same for the `15` hour-mark:

```{r, layout="l-body-outset", fig.width=8, fig.asp=0.618}
IPC_ret_tbl %>% 
  filter(tidy_hour == 15) %>% 
  count(tidy_minute, sort = T, name = "trades_per_minute") %>% 
  ggplot(aes(tidy_minute, trades_per_minute)) +
  geom_col(fill = amazing_colors[3]) +
  scale_x_continuous(breaks = seq(0, 60, 5)) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
    labs(
    title = "Number of Trades per Minute from 3:00PM-3:59PM",
    x = "Minute",
    y = NULL
  )
```

Here, the time with most trades is 3:00PM exactly. That's what we need because we'll ignore the rest of the trades. Following Liu, Patton, Sheppard (2013), we'll discard any prices that were quoted outside normal business hours. Hence, we want trades that lie between `08:30` and `15:00`. We'll store these trades in a new object called `IPC_tfr_tbl` (IPC time-filtered returns tibble). Finally, we'll print the earliest and latest times in our data to check.

```{r}
left_limit <- hms(seconds = 0,
                  minutes = 30,
                  hours = 8)

right_limit <- hms(seconds = 0,
                   minutes = 0,
                   hours = 15)

IPC_tfr_tbl <- IPC_ret_tbl %>%
  filter(tidy_time >= left_limit & tidy_time <= right_limit)

IPC_tfr_tbl
# Remove old df
# rm(IPC_ret_tbl)
```

```{r}
IPC_tfr_tbl %>% select(tidy_time) %>% unique() %>% dplyr::arrange(tidy_time) %>% head()
IPC_tfr_tbl %>% select(tidy_time) %>% unique() %>% dplyr::arrange(tidy_time) %>% tail()
```

We have filtered out `r nrow(IPC_ret_tbl) - nrow(IPC_tfr_tbl)` datapoints.

One last check we need to do regarding this point is the amount of different days that we just filtered. To start, let's see all unique dates before narrowing the time window:

```{r}
IPC_ret_tbl %>% group_by(tidy_date) %>% dplyr::count() %>% nrow()
```

Now let's check for all unique dates after narrowing the time window:

```{r}
IPC_tfr_tbl %>% group_by(tidy_date) %>% dplyr::count() %>% nrow()
```

Thus, we see that we have lost `r IPC_ret_tbl %>% group_by(tidy_date) %>% dplyr::count() %>% nrow() - IPC_tfr_tbl %>% group_by(tidy_date) %>% dplyr::count() %>% nrow()` days of data. For purposes of this analysis, we deem that to be valid.

## 2.3 Setting Overnight Returns to Zero

Another aspect that we want to take into account is: the fact that the overnight return should not be taken into account. Therefore, we set the first log-return of each day to zero. Once again, we store these values in a new object called `IPC_oar_tbl` (which stands for IPC overnight adjusted returns tibble).

```{r}
(IPC_oar_tbl <- IPC_tfr_tbl %>% 
   
  mutate(log_ret = ifelse(test = lag(tidy_date) == tidy_date, 
                          yes = log_ret, 
                          no = 0)) %>%
  
  mutate(day_change = ifelse(test = lag(tidy_date) == tidy_date, 
                             yes = 0, 
                             no = 1)))
# Remove old df
# rm(IPC_tfr_tbl)
```

Our first log-return should be zero, but we have `NA`. Let's check that this is the only `NA`:

```{r}
IPC_oar_tbl %>% 
  filter(is.na(log_ret)) %>% 
  select(tidy_date, log_ret)
```

It is indeed our only `NA`. We can set it to zero.

```{r}
IPC_oar_tbl$log_ret[1] <- 0

IPC_oar_tbl
```

We can check that all overnight returns are zero. We've created a variable (`day_change`) that goes to `1` when we have a change in day.

```{r}
IPC_oar_tbl %>% 
  filter(day_change == 1) %>% 
  select(log_ret) %>% 
  unique()
```

Moreover, the number of overnight returns that have been set to zero should be one less than the number of different dates we have for the `tidy_date` variable. Let's check if this holds true:

```{r}
IPC_oar_tbl %>% 
  filter(day_change == 1) %>% 
  select(trade_id, tidy_date, log_ret)
```

We have `5,607` datapoints where `day_change = 1`. This should be one less than the amount of different dates (one less because the first date doesn't have `day_change` set to one).

```{r}
IPC_oar_tbl %>% select(tidy_date) %>% unique() %>% nrow()
```

Thus, we see that this holds true. We have correctly set the overnight returns to zero.

## 2.4 Removing Outliers

We want to filter out log-returns whose absolute value is greater than some threshold. We'll do this again later for 5-minute returns and 30-minute returns. We define the threshold such that we are filtering keeping \~ 99% of the original data. Log-returns that are outside of the defined threshold are immediately set to zero.

```{r}
threshold <- 0.0012

IPC_fr <- IPC_oar_tbl %>% 
  
  # we keep values below the threshold
  mutate(log_ret = ifelse(test = abs(log_ret) < threshold, 
                          yes = log_ret, 
                          no = 0)
         )

# Remove old df
# rm(IPC_oar_tbl)
```

In this case we've set the variable `threshold` to the value (`0.0012`) such that we're filtering out \~ 1% of the data. We store the filtered data in the `IPC_fr` tibble (which stands for IPC filtered returns).

## 2.5 Padding the Time Series

Now, after all the data wrangling that we did, we still don't have a perfect time series, because it has some "holes" in it. We can check this by plotting, again, the number of log-returns we have per hour:

```{r, layout="l-body-outset", fig.width=8, fig.asp=0.618}
IPC_fr %>% mutate(tidy_hour = as.factor(tidy_hour)) %>% 
  count(tidy_hour, sort = T, name = "trades_per_hour") %>% 
  # mutate(tidy_hour = fct_reorder(tidy_hour, .x = trades_per_hour, .desc = T)) %>%
  ggplot(aes(tidy_hour, trades_per_hour)) +
  geom_col(fill = amazing_colors[1]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  labs(
    title = "No. Trades per Hour in the Sample",
    x = "Hour",
    y = NULL
  )
```

Most of the hours (9 - 14) have almost complete data. The hour number 8 has almost half the data points but that's natural since we only have half an hour there. However, almost all of the log-returns for the `15:00` mark seem to be missing. In normal business hours, we have 6 hours and a half of market trading. If we have one-minute trades, that means we should have $6.5 * 60 + 1 = 391$ trades/day. The `+1` in the equation comes from including the log-returns at exactly `15:00` hours.

```{r, echo = FALSE}
# timetk::tk_make_timeseries(start_date = "2020-09-11 08:30:00", by = "1 min", length_out = 391) %>% head()
```

Therefore, we'll pad the time series with the mean log-return for each half-hour.

```{r, eval = FALSE}
IPC_padded <-
  IPC_fr %>%
  
  # Create the half-hour variable
  mutate(
    tidy_half_hour = case_when(
      tidy_hour == 8 ~ 1,
      tidy_hour == 9 & tidy_minute < 30 ~ 2,
      tidy_hour == 9 & tidy_minute >= 30 ~ 3,
      tidy_hour == 10 & tidy_minute < 30 ~ 4,
      tidy_hour == 10 & tidy_minute >= 30 ~ 5,
      tidy_hour == 11 & tidy_minute < 30 ~ 6,
      tidy_hour == 11 & tidy_minute >= 30 ~ 7,
      tidy_hour == 12 & tidy_minute < 30 ~ 8,
      tidy_hour == 12 & tidy_minute >= 30 ~ 9,
      tidy_hour == 13 & tidy_minute < 30 ~ 10,
      tidy_hour == 13 & tidy_minute >= 30 ~ 11,
      tidy_hour == 14 & tidy_minute < 30 ~ 12,
      tidy_hour == 14 & tidy_minute >= 30 ~ 12,
      TRUE ~ 13
    )
  ) %>%
  
  # Create the tidy_dttm variable for padding
  mutate(tidy_dttm = lubridate::as_datetime(str_c(tidy_date, " ", tidy_time))) %>%
  
  # Pad the time series, starting from 1996-01-02 08:30:00
  timetk::pad_by_time(
    .date_var = "tidy_dttm",
    .by = "1 min",
    .start_date = as_datetime('1996-01-02 08:30:00'),
    .end_date = as_datetime('2018-06-05 15:00:00')
  )
```

```{r, eval = FALSE}
IPC_padded %>% dplyr::relocate(tidy_dttm, everything())
```

```{r, eval = FALSE}
IPC_padded_tbl <- 
  
  IPC_padded %>%
  
  # Filter out trades that are out of normal business hours
  # filter(tidy_time >= left_limit & tidy_time <= right_limit) %>% 

  # Create the time-related variables
  mutate(
    raw_time    = as_hms(tidy_dttm),
    tidy_date   = lubridate::ymd(str_sub(tidy_dttm, 1, 10)),
    tidy_year   = lubridate::year(tidy_date),
    tidy_month  = lubridate::month(tidy_date),
    tidy_mday   = lubridate::mday(tidy_date),
    tidy_wday   = lubridate::wday(tidy_date, label = T, locale = "English"),
    tidy_hour   = lubridate::hour(raw_time),
    tidy_minute = lubridate::minute(raw_time),
    tidy_time   = hms::as_hms(raw_time)
  ) %>%
  
  # Create the trade_id variable
  mutate(trade_id = seq(1:nrow(IPC_padded))) %>%
  
  # Drop the tidy_dttm variable
  select(-c(tidy_dttm, raw_time)) %>% 
  
  # Rearrange the order of the columns
  select(trade_id, tidy_date, tidy_time, log_ret, dplyr::everything())
```

# IPC and Volatility Indices -- Daily Data

The data was taken from a Bloomberg terminal and from [S&P Dow Jones Indices](https://spindices.com/indices/strategy/sp-bmv-ipc-vix).

```{r}
indices_data <- 
  
  # Read Excel file
  read_excel(path = here("01-Data", "mexbol-vimex.xlsx"),
             
             # Specify values for NA
             na = c("#NA", "#N/A N/A"),
             
             # Specify column type
             col_types = c("date", rep("numeric", 3))
             ) %>%
  
  # Set names for columns
  set_names(c("date", "mexbol_ipc", "vimex", "ipc_vix")) %>% 
  
  # Parse date column
  mutate(date = ymd(date))

indices_data
```

```{r}
# To-Do:

# 1. Transform data to long format

# 2. Re-do plots with facet_wrap
```

```{r}
indices_data %>% 
  pivot_longer(cols = contains("x"), )
```

Let's visualize the daily data for the IPC and the VIMEX:

```{r, layout="l-body-outset"}
# Build IPC plot
plot_ipc <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & VIMEX Volatility Index",
    subtitle = "(2005 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )


# Build VIMEX plot
plot_vimex <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, vimex)) +
  geom_line(colour = amazing_colors[2]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
    labs(
    x = NULL,
    y = "VIMEX"
  )

# Print plot
cowplot::plot_grid(plot_ipc, 
                   plot_vimex, 
                   ncol = 1,
                   rel_heights = c(1, 1))
```

We can do the same for the IPC and the IPC VIX:

```{r, layout="l-body-outset"}
# Build IPC plot with new title
plot_ipc2 <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & IPC VIX Volatility Index",
    subtitle = "(2015 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )
  

# Build IPC VIX plot
plot_ipcvix <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, ipc_vix)) +
  geom_line(colour = amazing_colors[2]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    x = NULL,
    y = "IPC VIX"
  )

# Print plot
cowplot::plot_grid(plot_ipc2, 
                   plot_ipcvix, 
                   ncol = 1,
                   rel_heights = c(1, 1))
```

We can repeat the plots but with a smooth average on top, so we see the trend along time:

```{r, layout="l-body-outset"}
# Build IPC plot
plot_ipc <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  geom_smooth(colour = amazing_colors[3]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & VIMEX Volatility Index",
    subtitle = "(2005 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )


# Build VIMEX plot
plot_vimex <- indices_data %>% 
  filter(!is.na(vimex)) %>% 
  ggplot(aes(date, vimex)) +
  geom_line(colour = amazing_colors[2]) +
  geom_smooth(colour = amazing_colors[4]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "2 years", labels = scales::date_format("%Y")) +
    labs(
    x = NULL,
    y = "VIMEX"
  )

# Print plot
cowplot::plot_grid(plot_ipc, 
                   plot_vimex, 
                   ncol = 1,
                   rel_heights = c(1, 1))
```

```{r, layout="l-body-outset"}
# Build IPC plot with new title
plot_ipc2 <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, mexbol_ipc)) +
  geom_line(colour = amazing_colors[1]) +
  geom_smooth(colour = amazing_colors[3]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    title = "IPC Index & IPC VIX Volatility Index",
    subtitle = "(2015 - 2019)",
    x = NULL,
    y = "S&P/BMV IPC"
  )
  

# Build IPC VIX plot
plot_ipcvix <- indices_data %>% 
  filter(!is.na(ipc_vix)) %>% 
  ggplot(aes(date, ipc_vix)) +
  geom_line(colour = amazing_colors[2]) +
  geom_smooth(colour = amazing_colors[4]) +
  scale_y_continuous(labels = scales::number_format(big.mark = ",")) +
  scale_x_date(breaks = "1 year", labels = scales::date_format("%Y")) +
  labs(
    x = NULL,
    y = "IPC VIX"
  )

# Print plot
cowplot::plot_grid(plot_ipc2, 
                          plot_ipcvix, 
                          ncol = 1,
                          rel_heights = c(1, 1))
```
